---
title: "Demonstration of the COMBO R Package"
author: 'Created by Kimberly A. Hochstedler. Contact: kah343@cornell.edu'
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE, error = FALSE, message = FALSE,
                      fig.align = "center")

library(ggplot2)
library(kableExtra)
```

\centering
![](/Users/hochsted/Dropbox/Misclassification/Code/RPackages/hex_stickers/COMBO_Hex_Sticker_cropped.png){width=30%}

\raggedright

In this vignette, we provide a demonstration of the R Package *COMBO* (correcting misclassified binary outcomes). This package provides methods for fitting logistic regression models when the binary outcome is potentially misclassified. Technical details about estimation are not included in this demonstration. For additional information on the methods used in this R Package, please consult ``Statistical inference for association studies in the presence of binary outcome misclassification" by Kimberly A. Hochstedler and Martin T. Wells. 


## Model and Conceptual Framework
Let $Y = j$ denote an observation's true outcome status, taking values $j \in \{1, 2\}$. Suppose we are interested in the relationship between $Y$ and a set of predictors, $X$, that are correctly measured. This relationship constitutes the \textit{true outcome mechanism}. Let $Y^* = k$ be the observed outcome status, taking values $k \in \{1,2\}$. $Y^*$ is a potentially misclassified version of $Y$. Let $Z$ denote a set of predictors related to sensitivity and specificity. The mechanism that generates the observed outcome, $Y^*$, given the true outcome, $Y$, is called the \textit{observation mechanism}. **Figure 1** displays the conceptual model. The following equations express the conceptual process mathematically.

$$\text{True outcome mechanism: } \text{logit}\{ P(Y = j | X ; \beta) \} = \beta_{j0} + \beta_{jX} X$$
$$\text{Observation mechanism: } \text{logit}\{ P(Y^* = k | Y = j, Z ; \gamma) \} = \gamma_{kj0} + \gamma_{kjZ} Z$$

\centering
![Conceptual Model](/Users/hochsted/Dropbox/Misclassification/Code/RPackages/binary_obs_data_structure.png)

\raggedright

## Simulate data
We begin this demonstration by generating data using the `COMBO_data()` function. The binary outcome data simulated by this scheme is subject to misclassification. The predictor related to the true outcome mechanism is "x" and the predictor related to the observation mechanism is "z". 
```{r}
library(COMBO)
library(dplyr)

# Set seed.
set.seed(123)

# Set sample size, x and z distribution information.
n <- 1000
x_mu <- 0
x_sigma <- 1
z_shape <- 1

# Set true parameter values.
true_beta <- matrix(c(1, -2), ncol = 1)
true_gamma <- matrix(c(.5, 1, -.5, -1), nrow = 2, byrow = FALSE)

# Generate data.
my_data <- COMBO_data(sample_size = n,
                      x_mu = x_mu, x_sigma = x_sigma,
                      z_shape = z_shape,
                      beta = true_beta, gamma = true_gamma)

# Save list elements as vectors.
Ystar <- my_data[["obs_Y"]]
x_matrix <- my_data[["x"]]
z_matrix <- my_data[["z"]]
```

## Effect estimation
We propose estimation methods using the Expectation-Maximization algorithm (EM) and Markov Chain Monte Carlo (MCMC). Each method checks and corrects instances of label switching, as described in Hochstedler and Wells (2022). In the code below, we provide functions for implementing these methods. 
```{r}
# Supply starting values for all parameters.
starting_values <- rep(1,6)
beta_start <- matrix(starting_values[1:2], ncol = 1)
gamma_start <- matrix(starting_values[3:6], ncol = 2, nrow = 2, byrow = FALSE)

# Estimate parameters using the EM-Algorithm.
EM_results <- COMBO_EM(Ystar, x_matrix = x_matrix, z_matrix = z_matrix,
                       beta_start = beta_start, gamma_start = gamma_start)

EM_results
```

```{r}
# Specify parameters for the prior distributions.
unif_lower_beta <- matrix(c(-5, -5, NA, NA), nrow = 2, byrow = TRUE)
unif_upper_beta <- matrix(c(5, 5, NA, NA), nrow = 2, byrow = TRUE)

unif_lower_gamma <- array(data = c(-5, NA, -5, NA, -5, NA, -5, NA),
                          dim = c(2,2,2))
unif_upper_gamma <- array(data = c(5, NA, 5, NA, 5, NA, 5, NA),
                          dim = c(2,2,2))

beta_prior_parameters <- list(lower = unif_lower_beta, upper = unif_upper_beta)
gamma_prior_parameters <- list(lower = unif_lower_gamma, upper = unif_upper_gamma)

# Estimate parameters using MCMC. 
MCMC_results <- COMBO_MCMC(Ystar, x = x_matrix, z = z_matrix,
                           prior = "uniform",
                           beta_prior_parameters = beta_prior_parameters,
                           gamma_prior_parameters = gamma_prior_parameters,
                           number_MCMC_chains = 4,
                           MCMC_sample = 2000, burn_in = 1000)

MCMC_results$posterior_means_df
MCMC_results$naive_posterior_means_df
```

### Plotting effect estimates
**Figure 2** shows the parameter estimates (+/- one standard deviation) for different analysis methods: EM, MCMC, SAMBA (an R package that estimates a binary outcome misclassification model, assuming perfect specificity), and a ``naive" logistic regression of $Y^* | X$. 
```{r, echo = FALSE}
MCMC_SD <- MCMC_results$posterior_sample_df %>%
  group_by(parameter) %>%
  summarise(SD = sd(sample)) %>%
  ungroup()
MCMC_results_df <- data.frame(Parameter = c("beta1", "beta2",
                                            "gamma11", "gamma12",
                                            "gamma21", "gamma22"),
                              Estimates = MCMC_results$posterior_means_df$posterior_mean,
                              SE = MCMC_SD$SD)

results_df <- rbind(EM_results[,-4], MCMC_results_df)

results_df$lower <- results_df$Estimates - results_df$SE
results_df$upper <- results_df$Estimates + results_df$SE
results_df$method <- c(rep("EM", 6), rep("SAMBA", 4),
                       rep("Perfect Sensitivity EM", 4),
                       rep("Naive", 2), rep("MCMC", 6))
results_df$Parameter <- c("beta1", "beta2", "gamma11", "gamma21", "gamma12", "gamma22",
                          "beta1", "beta2", "gamma11", "gamma21",
                          "beta1", "beta2", "gamma12", "gamma22",
                          "beta1", "beta2",
                          "beta1", "beta2", "gamma11", "gamma12", "gamma21", "gamma22")
results_df$place_holder <- 1
results_df$True_Value <- c(c(true_beta), c(true_gamma),
                           c(true_beta), c(true_gamma)[1:2],
                           c(true_beta), c(true_gamma)[3:4],
                           c(true_beta),
                           c(true_beta), true_gamma[1,], true_gamma[2,])

ggplot(data = results_df %>% filter(method != "Perfect Sensitivity EM")) +
  geom_hline(aes(yintercept = True_Value), linetype = "dashed") +
  geom_point(aes(y = Estimates, x = method, color = method), size = 2) +
  geom_linerange(aes(ymin = lower, ymax = upper, x = method, color = method)) +
  facet_grid(~Parameter, scales = "free") +
  theme_bw() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) +
  ggtitle("Parameter estimates across analysis methods",
          subtitle = "Dashed line denotes true parameter value.") +
  labs(x = "Parameter", y = "Estimate", color = "Method") +
  theme(legend.position = "bottom")
```

## Estimating sensitivity and specificity
For each analysis method, we may use the estimated $\gamma$ parameters to compute estimates of sensitivity and specificity as a function of the covariate, $z$. Here, we compute these values under the EM algorithm estimates, MCMC estimates, and using the generated data. 
```{r}
# Create matrix of gamma parameter estimates from the EM algorithm.
EM_gamma <- matrix(EM_results$Estimates[3:6], ncol = 2, byrow = FALSE)

# Compute misclassification probabilities.
EM_misclassification_prob <- misclassification_prob(EM_gamma,
                                                    matrix(z_matrix, ncol = 1))
# Find the average sensitivity and specificity. 
EM_sensitivity_df <- EM_misclassification_prob %>% 
  filter(Y == 1) %>% filter(Ystar == 1)
EM_sensitivity <- mean(EM_sensitivity_df$Probability)

EM_specificity_df <- EM_misclassification_prob %>% 
  filter(Y == 2) %>% filter(Ystar == 2)
EM_specificity <- mean(EM_specificity_df$Probability)
```

```{r}
# Create matrix of gamma parameter estimates from MCMC.
MCMC_gamma <- matrix(MCMC_results$posterior_means_df$posterior_mean[3:6],
                     ncol = 2, byrow = TRUE)

# Compute misclassification probabilities.
MCMC_misclassification_prob <- misclassification_prob(MCMC_gamma,
                                                      matrix(z_matrix, ncol = 1))

# Find the average sensitivity and specificity
MCMC_sensitivity_df <- MCMC_misclassification_prob %>% 
  filter(Y == 1) %>% filter(Ystar == 1)
MCMC_sensitivity <- mean(MCMC_sensitivity_df$Probability)

MCMC_specificity_df <- MCMC_misclassification_prob %>% 
  filter(Y == 2) %>% filter(Ystar == 2)
MCMC_specificity <- mean(MCMC_specificity_df$Probability)
```

```{r}
# Use the generated data to compute the actual sensitivity and specificity rate.
data_classification_table <- table(my_data[["obs_Y"]], my_data[["true_Y"]])

true_sensitivity <- prop.table(data_classification_table, 2)[1,1]

true_specificity <- prop.table(data_classification_table, 2)[2,2]
```

```{r, echo = FALSE}
misclass_results <- data.frame(Data = c(true_sensitivity, true_specificity),
                               EM = c(EM_sensitivity, EM_specificity),
                               MCMC = c(MCMC_sensitivity, MCMC_specificity)) %>%
  round(3)

kbl(t(misclass_results), col.names = c("Sensitivity, P(Y* = 1 | Y = 1)",
                                       "Specificity, P(Y* = 2 | Y = 2)"),
    booktabs = TRUE) %>%
  kable_styling(latex_options = "HOLD_position")
```
**Table 1** shows the actual sensitivity and specificity values for the data, in addition to the average sensitivity and specificity estimates computed from EM-Algorithm and MCMC parameter estimates and the covariate $z$. 

## References
Hochstedler, K.A. and Wells, M.T. ``Statistical inference for association studies in the presence of binary outcome misclassification", (2022). In preparation.